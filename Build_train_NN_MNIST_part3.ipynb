{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will add more layers and use activation functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1. Import Tensorflow and other helper libraries\n",
    "\n",
    "# make sure tensorflow is installed; uncomment the line before if you need to\n",
    "# pip install tensorflow\n",
    "\n",
    "# TensorFlow and tf.keras\n",
    "import tensorflow as tf\n",
    "\n",
    "# Helper libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MNIST database (Modified National Institute of Standards and Technology database) \n",
    "is a large database of handwritten digits that is commonly used for training various \n",
    "image processing systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "(10000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "# Step 2: load the MNIST data and convert pixel intensities to doubles\n",
    "# Explore the shape of the data\n",
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A Sequential model is appropriate for a plain stack of \n",
    "# layers where each layer has exactly one input tensor and one output tensor.\n",
    "# A Sequential model is not appropriate when:\n",
    "#  - Your model has multiple inputs or multiple outputs\n",
    "#  - Any of your layers has multiple inputs or multiple outputs\n",
    "#  - You need to do layer sharing\n",
    "#  - You want non-linear topology (e.g. a residual connection, a multi-branch model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Build the tf.keras.Sequential model by stacking the following three layers:\n",
    "\n",
    "# A. The first layer in the neural network takes input signals(values) and passes \n",
    "# them on to the next layer. It doesnâ€™t apply any operations on the input signals(values) \n",
    "# and has no weights and biases values associated. In our network the input signals \n",
    "# are of size 28 by 28\n",
    "# The first layer is of type \"Flatten\" and you can use an optional input shape \n",
    "# (the input images are 28 by 28)\n",
    "# Flattening is converting the data into a 1-dimensional array for input \n",
    "# into to the next layer.\n",
    "\n",
    "# B. The LAST layer is Dense (fully connected layer), the output shape is 1 x 10\n",
    "# The size of the output is 10 because we have 10 possible characters: 0,1,2,..,9\n",
    "\n",
    "# C. Add one fully connected layer, before the last layer.\n",
    "# IMPORTANT: \n",
    "# 1) In artificial neural networks, hidden layers are required if and only if the data must be\n",
    "# separated non-linearly (which you would usually know before you begin thinking of a NN).\n",
    "# 2) Empirically derived rules: the number of neurons in that layer is the mean of the neurons \n",
    "# in the input and output layers.\n",
    "# The number of neurons in the input layer = number of samples\n",
    "\n",
    "# Task: modify the sequential model you built in Part 2 by adding a \"relu\" activation function\n",
    "# to the second layer (this is the Dense layer with 392 neurons you added in Part 2)\n",
    "\n",
    "\n",
    "# Insert your code below:\n",
    "\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28,28)),\n",
    "    tf.keras.layers.Dense(392, activation='relu'),\n",
    "    tf.keras.layers.Dense(10)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten (Flatten)           (None, 784)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 392)               307720    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                3930      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 311,650\n",
      "Trainable params: 311,650\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Once a model is \"built\", you can call its summary() method to display its contents:\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.24884108,  0.19673565,  0.44082052, -0.17454691, -0.06626126,\n",
       "        -0.0504199 ,  0.02443631, -0.22840624, -0.19735771, -0.48858923]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For each example the model returns a vector of \"logits\" or \"log-odds\" scores, one for each class.\n",
    "# pass 1 training data image to the model and convert the predictions into a numpy array\n",
    "predictions = model(x_train[:1]).numpy()\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.08179351, 0.12771161, 0.16301782, 0.08810172, 0.09817757,\n",
       "        0.09974522, 0.10749833, 0.08348215, 0.0861148 , 0.06435726]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use the tf.nn.softmax function to convert these logits into \"probabilities\" for each class:\n",
    "tf.nn.softmax(predictions).numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.3051362"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Choose an optimizer and loss function for training\n",
    "\n",
    "# Deep learning neural networks are trained using the stochastic gradient descent optimization \n",
    "# algorithm. As part of the optimization algorithm, the error for the current state of the \n",
    "# model must be estimated repeatedly. This requires the choice of an error function, \n",
    "# conventionally called a loss function, that can be used to estimate the loss of the model so \n",
    "# that the weights can be updated to reduce the loss on the next evaluation.\n",
    "\n",
    "# The losses.SparseCategoricalCrossentropy loss takes a vector of logits and a True index and \n",
    "# returns a scalar loss for each example.\n",
    "\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "# This loss is equal to the negative log probability of the true class: It is zero if the model \n",
    "# is sure of the correct class. This untrained model gives probabilities close to random\n",
    "# (1/10 for each class), so the initial loss should be close to -tf.math.log(1/10) ~= 2.3.\n",
    "\n",
    "loss_fn(y_train[:1], predictions).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.2091 - accuracy: 0.9390\n",
      "Epoch 2/10\n",
      "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0850 - accuracy: 0.9743\n",
      "Epoch 3/10\n",
      "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0554 - accuracy: 0.9821\n",
      "Epoch 4/10\n",
      "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0386 - accuracy: 0.9880\n",
      "Epoch 5/10\n",
      "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0298 - accuracy: 0.9906\n",
      "Epoch 6/10\n",
      "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0227 - accuracy: 0.9926\n",
      "Epoch 7/10\n",
      "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0173 - accuracy: 0.9941\n",
      "Epoch 8/10\n",
      "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0141 - accuracy: 0.9954\n",
      "Epoch 9/10\n",
      "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0135 - accuracy: 0.9956\n",
      "Epoch 10/10\n",
      "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0099 - accuracy: 0.9966\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x13236ff58b0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 4: Ready to compile. optimizer parameter = 'adam'. Other optimizer options here: \n",
    "# https://www.tensorflow.org/api_docs/python/tf/keras/optimizers\n",
    "# loss = the name of the loss function\n",
    "# Typically you will use metrics=['accuracy']\n",
    "model.compile(optimizer='adam',\n",
    "              loss=loss_fn,\n",
    "              metrics=['accuracy'])\n",
    "# The Model.fit method adjusts the model parameters to minimize the loss\n",
    "\n",
    "# Task: Call the model.fit method to train the model for 10 iterations\n",
    "\n",
    "# Insert your code below:\n",
    "\n",
    "\n",
    "model.fit(x_train, y_train, epochs=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 - 1s - loss: 0.0742 - accuracy: 0.9819 - 925ms/epoch - 3ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.07424212992191315, 0.9818999767303467]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 5: Evaluate the model: compare how the model performs on the test dataset\n",
    "\n",
    "# Task: Use the Model.evaluate method to check the model's performanceon the test \n",
    "# set (x_test, y_test). It would be useful to print the model's testing accuracy as well.\n",
    "\n",
    "# Insert your code below:\n",
    "\n",
    "\n",
    "\n",
    "model.evaluate(x_test, y_test, verbose=2)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
